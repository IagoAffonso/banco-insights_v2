"""
ğŸ¦ Production Data Transformation Script - Fixed Version

This script safely processes the actual consolidated_cleaned.csv file.
"""

import pandas as pd
import numpy as np
from pathlib import Path
import json
import os
import warnings

warnings.filterwarnings('ignore')

def safe_process_key_tables():
    """
    Process only the most critical tables first to test the system.
    """
    
    print("ğŸ¦ BANCO INSIGHTS 2.0 - SAFE PRODUCTION TRANSFORMATION")
    print("=" * 60)
    
    source_file = "bacen_project_v1/data/consolidated_cleaned.csv"
    
    if not os.path.exists(source_file):
        print(f"âŒ Source file not found: {source_file}")
        return False
    
    # Load unique identifiers
    try:
        with open('EDA/unique_values.json', 'r') as f:
            unique_values = json.load(f)
        available_ids = set(unique_values.get('NomeRelatorio_Grupo_Coluna', []))
        print(f"ğŸ“Š Loaded {len(available_ids)} unique identifiers")
    except Exception as e:
        print(f"âŒ Error loading identifiers: {e}")
        return False
    
    # Define key tables to process
    key_tables = {
        'resumo_quarterly': {
            'Ativo_Total': 'Resumo_nagroup_Ativo Total',
            'Patrimonio_Liquido': 'Resumo_nagroup_PatrimÃ´nio LÃ­quido',
            'Lucro_Liquido': 'Resumo_nagroup_Lucro LÃ­quido',
            'Captacoes': 'Resumo_nagroup_CaptaÃ§Ãµes'
        },
        'credito_clientes_operacoes_quarterly': {
            'Quantidade_de_Clientes_com_Operacoes_Ativas': 'Carteira de crÃ©dito ativa - quantidade de clientes e de operaÃ§Ãµes_nagroup_Quantidade de clientes com operaÃ§Ãµes ativas',
            'Quantidade_de_Operacoes_Ativas': 'Carteira de crÃ©dito ativa - quantidade de clientes e de operaÃ§Ãµes_nagroup_Quantidade de operaÃ§Ãµes ativas'
        }
    }
    
    # Process each table
    results = {}
    
    for table_name, metrics in key_tables.items():
        print(f"\\nğŸ”„ Processing {table_name}...")
        
        # Validate metrics exist
        valid_metrics = {}
        for clean_name, identifier in metrics.items():
            if identifier in available_ids:
                valid_metrics[clean_name] = identifier
            else:
                print(f"   âš ï¸  Missing: {clean_name}")
        
        if not valid_metrics:
            print(f"   âŒ No valid metrics found for {table_name}")
            results[table_name] = False
            continue
        
        print(f"   âœ… Found {len(valid_metrics)} valid metrics")
        
        # Process in chunks
        try:
            chunk_results = []
            chunk_size = 50000
            total_chunks = 0
            relevant_rows = 0
            
            chunk_iter = pd.read_csv(
                source_file,
                chunksize=chunk_size,
                dtype={'CodInst': str, 'Saldo': float}
            )
            
            for chunk_num, chunk in enumerate(chunk_iter, 1):
                total_chunks = chunk_num
                
                # Filter for our metrics
                relevant = chunk[
                    chunk['NomeRelatorio_Grupo_Coluna'].isin(valid_metrics.values())
                ].copy()
                
                if len(relevant) == 0:
                    continue
                
                relevant_rows += len(relevant)
                
                # Create reverse mapping
                reverse_map = {v: k for k, v in valid_metrics.items()}
                relevant['Clean_Metric'] = relevant['NomeRelatorio_Grupo_Coluna'].map(reverse_map)
                
                # Pivot to wide format
                try:
                    pivoted = relevant.pivot_table(
                        index=['CodInst', 'NomeInstituicao', 'AnoMes', 'AnoMes_Q', 'AnoMes_Y'],
                        columns='Clean_Metric',
                        values='Saldo',
                        aggfunc='first'
                    ).reset_index()
                    
                    pivoted.columns.name = None
                    
                    if len(pivoted) > 0:
                        chunk_results.append(pivoted)
                        
                except Exception as e:
                    print(f"   âš ï¸  Error pivoting chunk {chunk_num}: {e}")
                
                if chunk_num % 20 == 0:
                    print(f"     ğŸ“ˆ Processed {chunk_num} chunks, {relevant_rows:,} relevant rows")
            
            print(f"   ğŸ“Š Summary: {total_chunks:,} chunks, {relevant_rows:,} relevant rows")
            
            # Combine results
            if chunk_results:
                final_df = pd.concat(chunk_results, ignore_index=True)
                
                # Remove duplicates
                initial_rows = len(final_df)
                final_df = final_df.drop_duplicates(subset=['CodInst', 'AnoMes'], keep='last')
                
                print(f"   ğŸ”§ Deduplication: {initial_rows:,} â†’ {len(final_df):,} rows")
                print(f"   ğŸ›ï¸  Institutions: {final_df['CodInst'].nunique():,}")
                print(f"   ğŸ“… Date range: {final_df['AnoMes_Q'].min()} to {final_df['AnoMes_Q'].max()}")
                
                # Save result
                output_path = Path("data_restructured/core_tables") / f"{table_name}.csv"
                output_path.parent.mkdir(parents=True, exist_ok=True)
                final_df.to_csv(output_path, index=False)
                
                print(f"   ğŸ’¾ Saved: {output_path}")
                results[table_name] = True
            else:
                print(f"   âŒ No data found")
                results[table_name] = False
                
        except Exception as e:
            print(f"   âŒ Error processing {table_name}: {e}")
            results[table_name] = False
    
    # Summary
    successful = sum(results.values())
    total = len(results)
    
    print(f"\\nğŸ“‹ TRANSFORMATION SUMMARY")
    print(f"   â€¢ Success rate: {successful}/{total} ({successful/total*100:.1f}%)")
    
    if successful == total:
        print("   ğŸ‰ ALL KEY TABLES PROCESSED SUCCESSFULLY!")
        return True
    else:
        failed = [k for k, v in results.items() if not v]
        print(f"   âš ï¸  Failed tables: {', '.join(failed)}")
        return False

if __name__ == "__main__":
    success = safe_process_key_tables()
    
    if success:
        print("\\nğŸš€ Next steps ready:")
        print("   1. âœ… Core tables created")
        print("   2. ğŸ”„ Ready for TTM calculations")
        print("   3. ğŸ”„ Ready for sanity checks")
    else:
        print("\\nâŒ Review errors above before proceeding")